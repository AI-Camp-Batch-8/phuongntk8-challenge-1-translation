{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ./scripts/ -name \"*.sh\" -exec chmod +x {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/clear.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the corpus...\n",
      "Tokenizer Version 1.1\n",
      "Language: vi\n",
      "Number of threads: 1\n",
      "WARNING: No known abbreviations for language 'vi', attempting fall-back to English version...\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n",
      "Training truecase...\n",
      "Truecasing the data...\n",
      "Limiting the sentence length to 80...\n",
      "clean-corpus.perl: processing /home/haki/qai-hackathon-smt/corpus/train.true.vi & .en to /home/haki/qai-hackathon-smt/corpus/train.clean, cutoff 1-80, ratio 9\n",
      ".\n",
      "Input sentences: 12168  Output sentences:  12167\n"
     ]
    }
   ],
   "source": [
    "!scripts/01_corpus-prep.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the LM-corpus...\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n",
      "Training LM-truecase...\n",
      "Truecasing the LM-data...\n",
      "Training LM...\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/haki/qai-hackathon-smt/corpus/language-model.true.en\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "tcmalloc: large alloc 1981235200 bytes == 0x2f82000 @ \n",
      "tcmalloc: large alloc 7924940800 bytes == 0x790f6000 @ \n",
      "****************************************************************************************************\n",
      "Unigram tokens 42482 types 2836\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:34032 2:1690434816 3:3169565440 4:5071304704\n",
      "tcmalloc: large alloc 5071306752 bytes == 0x2f82000 @ \n",
      "tcmalloc: large alloc 1690435584 bytes == 0x1313f4000 @ \n",
      "tcmalloc: large alloc 3169566720 bytes == 0x251f6c000 @ \n",
      "Statistics:\n",
      "1 2836 D1=0.661207 D2=1.2 D3+=1.25225\n",
      "2 13715 D1=0.758296 D2=1.12775 D3+=1.21923\n",
      "3 22123 D1=0.815173 D2=1.14076 D3+=1.40101\n",
      "4 21138 D1=0.767112 D2=1.32855 D3+=1.65168\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 1283 assuming -p 1.5\n",
      "probing 1504 assuming -r models -p 1.5\n",
      "trie     571 without quantization\n",
      "trie     311 assuming -q 8 -b 8 quantization \n",
      "trie     537 assuming -a 22 array pointer compression\n",
      "trie     277 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:34032 2:219440 3:442460 4:507312\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:34032 2:219440 3:442460 4:507312\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:13072892 kB\tVmRSS:19304 kB\tRSSMax:1960156 kB\tuser:0.412867\tsys:0.798209\tCPU:1.21108\treal:1.23546\n",
      "Reading language-model.arpa.en\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!scripts/02_train-language-model.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "scripts/03_train-translation-model.sh: 13: Syntax error: Bad fd number\n"
     ]
    }
   ],
   "source": [
    "# Do NOT run this cell\n",
    "# Running this script from a notebook will result in an error as the output cell below. Just open the terminal and run the command below instead.\n",
    "\n",
    "# scripts/03_train-translation-model.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the script 03_train-translation-model.sh, you will see a new folder called \"models\" in the root directory of the project. This folder contains the trained models. The training process is done when you see the file workking/model/moses.ini generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: vi\n",
      "Number of threads: 1\n",
      "WARNING: No known abbreviations for language 'vi', attempting fall-back to English version...\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n",
      "Testing the model...\n"
     ]
    }
   ],
   "source": [
    "!scripts/04_test.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 39.06, 68.8/44.2/31.1/24.7 (BP=1.000, ratio=1.075, hyp_len=5716, ref_len=5319)\n"
     ]
    }
   ],
   "source": [
    "# This cell check performance of the trained model. BLEU score is a metric for evaluate translation performance.\n",
    "!cat corpus/BLEU-report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pls enter directory name to save some data and BLEU report for the given dataset: \n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell. \n",
    "# Open terminal and run the command below.\n",
    "# You can input a descriptive name like 'baseline' or whatever you want.\n",
    "\n",
    "# scripts/05_report.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning the model...\n",
      "Tokenizer Version 1.1\n",
      "Language: vi\n",
      "Number of threads: 1\n",
      "WARNING: No known abbreviations for language 'vi', attempting fall-back to English version...\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell. \n",
    "# Open terminal and run the command below.\n",
    "\n",
    "# scripts/06_tune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: vi\n",
      "Number of threads: 1\n",
      "WARNING: No known abbreviations for language 'vi', attempting fall-back to English version...\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 1\n",
      "Testing the model after tuning...\n"
     ]
    }
   ],
   "source": [
    "# The tuning process is done when you see the file working/mert-work/moses.ini generated.\n",
    "\n",
    "# Now you can test the tuned model by running this cell\n",
    "!scripts/07_test-tune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 42.00, 71.3/47.2/34.3/28.1 (BP=0.989, ratio=0.989, hyp_len=5262, ref_len=5319)\n"
     ]
    }
   ],
   "source": [
    "# Check performance after tuning \n",
    "!cat corpus/BLEU-report-tune.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pls enter directory name to save some data and BLEU report for the given dataset: \n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell. \n",
    "# Open terminal and run the command below.\n",
    "# You can input a descriptive name like 'baseline_tune' or whatever you want.\n",
    "\n",
    "# scripts/08_report-tune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've already trained a baseline model and tuned it. Let's use the tuned model to translate the private test. Place the private test file in the folder 'predict' and name it input.vi. Then run the this cell.\n",
    "\n",
    "!scripts/09_predict.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to train a new model with new data, you can run the following cells to clear the previous data before treaining a new model.\n",
    "!scripts/10_clear.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
